{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMcX6lfS6nItVaa/IoAEJJZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Castorres/Memory-Power-Performance-Analysis-of-Transformer-on-a-traditional-computer/blob/main/Memory_Power_Performance_Analysis_of_Transformer_on_a_traditional_computer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_HpBFMWNytZ",
        "outputId": "ce995cfd-2a8d-4934-9d48-34ae09182c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O tiny_shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
      ],
      "metadata": {
        "id": "1-0zgLhbPoM3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh tiny_shakespeare.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGbY0S10QHaX",
        "outputId": "bdfb12ce-7083-4b6b-c491-73387e2c64d2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 1.1M Sep 30 18:24 tiny_shakespeare.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "#reads the text ^^^\n",
        "# open :this opens the file, r :makes it so its only reading no editing\n",
        "# encoding :makes sure special characters are handled properly\n",
        "# .read() : it reads the enter files as one string\n",
        "# this results in var text contatining the entire file\n",
        "\n",
        "split = int(0.9 * len(text))\n",
        "# len(text) :this counts the total chars in the text\n",
        "# .9 * len(text) :calculates 90% of the text (will be used for training)\n",
        "# int() :converts into a integer(to slice indexes must be whole numbers)\n",
        "# this resulrs in var split being an index used to cut text into train and validation parts\n",
        "\n",
        "train_text = text [:split]\n",
        "val_text = text[split:]\n",
        "# text[:split] : this takes all the chars from the start to split, 90%\n",
        "# text[split:] : this takes all the chars from split to end, 10%\n",
        "# this results in var train_text which will train the model\n",
        "# and var val_text which will test the model's accuracy\n",
        "\n",
        "#splits it into 90% train / 10% validation^^^\n",
        "\n",
        "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(train_text)\n",
        "# with open( , \"w\") as f : this opens a new file named train.txt in write mode\n",
        "#f.write(train_text) : saves the training text into the new file\n",
        "#using with will have the file close one it is done\n",
        "\n",
        "with open(\"val.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(val_text)\n",
        "#this will the same as the code before it but for the val text\n",
        "\n",
        "#this saves them as new files^^^\n",
        "\n",
        "print(\"training characters:\", len(train_text))\n",
        "print(\"validation characters:\", len(val_text))\n",
        "# len(train_text) : counts the chars in the training set\n",
        "# len(val_text) : counts the chars in the val set\n",
        "#check character counts^^^"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyhxRr-pRRlg",
        "outputId": "7f2fd52f-0a11-4cfc-b012-b621b3403ec7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training characters: 1003854\n",
            "validation characters: 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = open(\"train.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "val_text = open(\"val.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "# open(...,\"r\",encoding) :this opens the files and using UTF-8 to protect special chars\n",
        "# .read() : this loads the file into one string - train_text\n",
        "# the same is done for val_text\n",
        "#reads training data^^^\n",
        "\n",
        "chars = sorted(list(set(train_text)))\n",
        "# set(train_text) :this is a set the keeps only unique chars (no duplicates)\n",
        "# list(..) :this converts the set back into a list (allowing it to be ordered)\n",
        "# sorted(..) : sorts the chars by Unicode code point making the index stable\n",
        "vocab_size = len(chars)\n",
        "# vocab_size = len : this counts how many distinct chars are in the training text\n",
        "#creates vocabulary: unique set of characters^^^\n",
        "\n",
        "print(\"Characters in vocab:\", chars)\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdgDjCq3WsTD",
        "outputId": "818d4ca2-21ce-4bbd-90e8-7f0826a39ebe"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters in vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Vocabulary size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch: i for i, ch in enumerate(chars)}  #string to index\n",
        "itos = {i: ch for i, ch in enumerate(chars)}  #index to string\n",
        "# enumerate(chars) :this yields (index, character) pairs like(1,' ')\n",
        "#create mappings, char -> int and int -> char^^^\n",
        "\n",
        "def encode(s):\n",
        "  \"\"\"Encodes a string into a list of intergers\"\"\"\n",
        "  return [stoi[ch] for ch in s]\n",
        "# encode(s) :list comprehension that replaces each char ch in s with its integer\n",
        "def decode(l):\n",
        "  \"\"\"Decodes a list of intergers back into a string\"\"\"\n",
        "  return ''.join([itos[i] for i in l])\n",
        "# decode() :turns chars back into a string using reverse mapping\n",
        "\n",
        "print(\"Encoded:\", encode(\"ROMEO\"))\n",
        "print(\"Decoded:\", decode(encode(\"ROMEO\")))\n",
        "#quick test^^"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDFAx85tYYBq",
        "outputId": "e2f9898a-2427-4ab8-8d17-b721353211f1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: [30, 27, 25, 17, 27]\n",
            "Decoded: ROMEO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#this loads PyTorch\n",
        "\n",
        "train_data = torch.tensor(encode(train_text), dtype=torch.long)\n",
        "# encode() :turns the training string into a list of ints, these are the token IDs\n",
        "# torch.tensor() :this converts the list into 1D tensor of 64-bit ints\n",
        "val_data = torch.tensor(encode(val_text), dtype=torch.long)\n",
        "# same goes for val_data\n",
        "#converts the entire dataset into torch tensors^^^\n",
        "\n",
        "print(\"Training data tensor shape:\", train_data.shape)\n",
        "print(\"validation data tensor shape:\", val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jHu845W5Awn",
        "outputId": "f5e7665e-26ea-4c48-dc3c-5fe0b7c664ba"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data tensor shape: torch.Size([1003854])\n",
            "validation data tensor shape: torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64     #this is how many sequences are per batch (64)\n",
        "block_size = 128    # this is the length of each sequence (128 chars)\n",
        "\n",
        "def get_batch(split):   #this will randoming pick where to start from either train_data or val_data\n",
        "  data = train_data if split == 'train' else val_data   #this will pick a 1D tensor from the token IDs\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])   #this is the input sequences\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])   #this is the target sequences\n",
        "  # this will be how the model is trained to predict the next char\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "ZOmZtwLikIlI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir -p /content/transformer_shakespeare/src /content/transformer_shakespeare/data /content/transformer_shakespeare/logs /content/transformer_shakespeare/output"
      ],
      "metadata": {
        "id": "pNDc6nTjbivg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv train.txt val.txt tiny_shakespeare.txt /content/transformer_shakespeare/data/"
      ],
      "metadata": {
        "id": "FoxXvmdCbkgd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/transformer_shakespeare/src/model.py\n",
        "\n",
        "import torch    #this is the main PyTorch libary\n",
        "import torch.nn as nn   #this has prebuilt neural network layers (linear, embedding, layerNorm)\n",
        "import torch.nn.functional as F   #this has lower level functions that are used inside models\n",
        "\n",
        "batch_size = 64\n",
        "block_size = 128\n",
        "n_embd = 256    #this embeds simension\n",
        "#this is the hidden size of vectors\n",
        "n_head = 4     #this is the number of attention heads\n",
        "n_layer = 4   #this is the number of transform layers\n",
        "dropout = 0.1 #this prevents overfitting\n",
        "#these hyperparameters will be used later in train.py^^^\n",
        "\n",
        "#this is the transformer model\n",
        "class TransformerModel(nn.Module):\n",
        "  #nn.Module : this is the base class for all neural nets\n",
        "  def __init__(self, vocab_size):\n",
        "  #vocab_size : this is the number of unique tokens in the dataset\n",
        "      super().__init__()\n",
        "\n",
        "      #1) the token and positional embeddings\n",
        "      self.token_embedding = nn.Embedding(vocab_size,n_embd)\n",
        "      #this will convert token IDs into vectors\n",
        "      self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "      #this will make positional encoding making it so each position gets a vector\n",
        "\n",
        "      #2) the transformer blocks\n",
        "      self.blocks = nn.Sequential(*[nn.Identity() for _ in range(n_layer)])\n",
        "      #nn.itenity is a placeholder, will be replaced with real attention + MLP blocks\n",
        "\n",
        "      #3) the layernorm before final projection\n",
        "      self.ln_f = nn.LayerNorm(n_embd)  #this stabilizes the training\n",
        "\n",
        "      #4) Final linear layer (projects to vocab size)\n",
        "      self.head = nn.Linear(n_embd, vocab_size) #this projects back to vocab, logits for next-char prediction\n",
        "\n",
        "  def forward(self, idx):\n",
        "    B, T = idx.shape    #this is the batch size and sequence length\n",
        "\n",
        "    #these are the token + position embeddings\n",
        "    tok_emb = self.token_embedding(idx)   #(B, T, n_embd)\n",
        "    pos_emb = self.position_embedding(torch.arange(T, device=idx.device)) #(T, n_embd)\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    #transformer blocks, current placeholders\n",
        "    x = self.block(x)\n",
        "\n",
        "    #final normalization and output\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.head(x)   #(B, T, vocab_size)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wDsk1AhFIuK",
        "outputId": "c916f963-d314-495a-e5a5-ab8b0f84c084"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/transformer_shakespeare/src/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/transformer_shakespeare/src && python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsUSm58b17b8",
        "outputId": "39b2af2f-4ac5-49cd-f2b2-cb8a89b1cb0b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 | Train Loss: 4.3657 | Val Loss: 3.9945\n",
            "Epoch 1/100 | Train Loss: 4.3657\n",
            "Epoch 2/100 | Train Loss: 3.9734 | Val Loss: 3.7235\n",
            "Epoch 2/100 | Train Loss: 3.9734\n",
            "Epoch 3/100 | Train Loss: 3.7212 | Val Loss: 3.5422\n",
            "Epoch 3/100 | Train Loss: 3.7212\n",
            "Epoch 4/100 | Train Loss: 3.5789 | Val Loss: 3.4522\n",
            "Epoch 4/100 | Train Loss: 3.5789\n",
            "Epoch 5/100 | Train Loss: 3.4464 | Val Loss: 3.3477\n",
            "Epoch 5/100 | Train Loss: 3.4464\n",
            "Epoch 6/100 | Train Loss: 3.3161 | Val Loss: 3.3314\n",
            "Epoch 6/100 | Train Loss: 3.3161\n",
            "Epoch 7/100 | Train Loss: 3.2702 | Val Loss: 3.2870\n",
            "Epoch 7/100 | Train Loss: 3.2702\n",
            "Epoch 8/100 | Train Loss: 3.2673 | Val Loss: 3.2392\n",
            "Epoch 8/100 | Train Loss: 3.2673\n",
            "Epoch 9/100 | Train Loss: 3.2173 | Val Loss: 3.1984\n",
            "Epoch 9/100 | Train Loss: 3.2173\n",
            "Epoch 10/100 | Train Loss: 3.1656 | Val Loss: 3.1823\n",
            "Epoch 10/100 | Train Loss: 3.1656\n",
            "Epoch 11/100 | Train Loss: 3.1509 | Val Loss: 3.1660\n",
            "Epoch 11/100 | Train Loss: 3.1509\n",
            "Epoch 12/100 | Train Loss: 3.1278 | Val Loss: 3.1406\n",
            "Epoch 12/100 | Train Loss: 3.1278\n",
            "Epoch 13/100 | Train Loss: 3.1188 | Val Loss: 3.1001\n",
            "Epoch 13/100 | Train Loss: 3.1188\n",
            "Epoch 14/100 | Train Loss: 3.1084 | Val Loss: 3.1023\n",
            "Epoch 14/100 | Train Loss: 3.1084\n",
            "Epoch 15/100 | Train Loss: 3.0831 | Val Loss: 3.1495\n",
            "Epoch 15/100 | Train Loss: 3.0831\n",
            "Epoch 16/100 | Train Loss: 3.0695 | Val Loss: 3.0185\n",
            "Epoch 16/100 | Train Loss: 3.0695\n",
            "Epoch 17/100 | Train Loss: 3.0305 | Val Loss: 3.0360\n",
            "Epoch 17/100 | Train Loss: 3.0305\n",
            "Epoch 18/100 | Train Loss: 3.0561 | Val Loss: 2.9951\n",
            "Epoch 18/100 | Train Loss: 3.0561\n",
            "Epoch 19/100 | Train Loss: 3.0204 | Val Loss: 2.9697\n",
            "Epoch 19/100 | Train Loss: 3.0204\n",
            "Epoch 20/100 | Train Loss: 2.9318 | Val Loss: 2.9783\n",
            "Epoch 20/100 | Train Loss: 2.9318\n",
            "Epoch 21/100 | Train Loss: 2.9348 | Val Loss: 2.8779\n",
            "Epoch 21/100 | Train Loss: 2.9348\n",
            "Epoch 22/100 | Train Loss: 2.9152 | Val Loss: 2.9212\n",
            "Epoch 22/100 | Train Loss: 2.9152\n",
            "Epoch 23/100 | Train Loss: 2.9067 | Val Loss: 2.9380\n",
            "Epoch 23/100 | Train Loss: 2.9067\n",
            "Epoch 24/100 | Train Loss: 2.8849 | Val Loss: 2.8685\n",
            "Epoch 24/100 | Train Loss: 2.8849\n",
            "Epoch 25/100 | Train Loss: 2.8089 | Val Loss: 2.8475\n",
            "Epoch 25/100 | Train Loss: 2.8089\n",
            "Epoch 26/100 | Train Loss: 2.8454 | Val Loss: 2.8916\n",
            "Epoch 26/100 | Train Loss: 2.8454\n",
            "Epoch 27/100 | Train Loss: 2.8457 | Val Loss: 2.8719\n",
            "Epoch 27/100 | Train Loss: 2.8457\n",
            "Epoch 28/100 | Train Loss: 2.8387 | Val Loss: 2.8106\n",
            "Epoch 28/100 | Train Loss: 2.8387\n",
            "Epoch 29/100 | Train Loss: 2.8211 | Val Loss: 2.8031\n",
            "Epoch 29/100 | Train Loss: 2.8211\n",
            "Epoch 30/100 | Train Loss: 2.8486 | Val Loss: 2.8037\n",
            "Epoch 30/100 | Train Loss: 2.8486\n",
            "Epoch 31/100 | Train Loss: 2.7954 | Val Loss: 2.8077\n",
            "Epoch 31/100 | Train Loss: 2.7954\n",
            "Epoch 32/100 | Train Loss: 2.8120 | Val Loss: 2.8057\n",
            "Epoch 32/100 | Train Loss: 2.8120\n",
            "Epoch 33/100 | Train Loss: 2.7693 | Val Loss: 2.7644\n",
            "Epoch 33/100 | Train Loss: 2.7693\n",
            "Epoch 34/100 | Train Loss: 2.7715 | Val Loss: 2.7351\n",
            "Epoch 34/100 | Train Loss: 2.7715\n",
            "Epoch 35/100 | Train Loss: 2.7143 | Val Loss: 2.7642\n",
            "Epoch 35/100 | Train Loss: 2.7143\n",
            "Epoch 36/100 | Train Loss: 2.7495 | Val Loss: 2.7325\n",
            "Epoch 36/100 | Train Loss: 2.7495\n",
            "Epoch 37/100 | Train Loss: 2.7346 | Val Loss: 2.7248\n",
            "Epoch 37/100 | Train Loss: 2.7346\n",
            "Epoch 38/100 | Train Loss: 2.7464 | Val Loss: 2.7364\n",
            "Epoch 38/100 | Train Loss: 2.7464\n",
            "Epoch 39/100 | Train Loss: 2.7263 | Val Loss: 2.7250\n",
            "Epoch 39/100 | Train Loss: 2.7263\n",
            "Epoch 40/100 | Train Loss: 2.6712 | Val Loss: 2.6868\n",
            "Epoch 40/100 | Train Loss: 2.6712\n",
            "Epoch 41/100 | Train Loss: 2.7141 | Val Loss: 2.6681\n",
            "Epoch 41/100 | Train Loss: 2.7141\n",
            "Epoch 42/100 | Train Loss: 2.6797 | Val Loss: 2.6785\n",
            "Epoch 42/100 | Train Loss: 2.6797\n",
            "Epoch 43/100 | Train Loss: 2.6736 | Val Loss: 2.7133\n",
            "Epoch 43/100 | Train Loss: 2.6736\n",
            "Epoch 44/100 | Train Loss: 2.6922 | Val Loss: 2.6722\n",
            "Epoch 44/100 | Train Loss: 2.6922\n",
            "Epoch 45/100 | Train Loss: 2.6447 | Val Loss: 2.6273\n",
            "Epoch 45/100 | Train Loss: 2.6447\n",
            "Epoch 46/100 | Train Loss: 2.6462 | Val Loss: 2.6858\n",
            "Epoch 46/100 | Train Loss: 2.6462\n",
            "Epoch 47/100 | Train Loss: 2.6415 | Val Loss: 2.6372\n",
            "Epoch 47/100 | Train Loss: 2.6415\n",
            "Epoch 48/100 | Train Loss: 2.6648 | Val Loss: 2.6187\n",
            "Epoch 48/100 | Train Loss: 2.6648\n",
            "Epoch 49/100 | Train Loss: 2.6262 | Val Loss: 2.6294\n",
            "Epoch 49/100 | Train Loss: 2.6262\n",
            "Epoch 50/100 | Train Loss: 2.6581 | Val Loss: 2.6405\n",
            "Epoch 50/100 | Train Loss: 2.6581\n",
            "Epoch 51/100 | Train Loss: 2.6011 | Val Loss: 2.6047\n",
            "Epoch 51/100 | Train Loss: 2.6011\n",
            "Epoch 52/100 | Train Loss: 2.5982 | Val Loss: 2.6221\n",
            "Epoch 52/100 | Train Loss: 2.5982\n",
            "Epoch 53/100 | Train Loss: 2.6258 | Val Loss: 2.6086\n",
            "Epoch 53/100 | Train Loss: 2.6258\n",
            "Epoch 54/100 | Train Loss: 2.6205 | Val Loss: 2.5587\n",
            "Epoch 54/100 | Train Loss: 2.6205\n",
            "Epoch 55/100 | Train Loss: 2.6241 | Val Loss: 2.6244\n",
            "Epoch 55/100 | Train Loss: 2.6241\n",
            "Epoch 56/100 | Train Loss: 2.5913 | Val Loss: 2.5880\n",
            "Epoch 56/100 | Train Loss: 2.5913\n",
            "Epoch 57/100 | Train Loss: 2.5874 | Val Loss: 2.5676\n",
            "Epoch 57/100 | Train Loss: 2.5874\n",
            "Epoch 58/100 | Train Loss: 2.5920 | Val Loss: 2.5764\n",
            "Epoch 58/100 | Train Loss: 2.5920\n",
            "Epoch 59/100 | Train Loss: 2.5887 | Val Loss: 2.5861\n",
            "Epoch 59/100 | Train Loss: 2.5887\n",
            "Epoch 60/100 | Train Loss: 2.5653 | Val Loss: 2.5517\n",
            "Epoch 60/100 | Train Loss: 2.5653\n",
            "Epoch 61/100 | Train Loss: 2.5737 | Val Loss: 2.5749\n",
            "Epoch 61/100 | Train Loss: 2.5737\n",
            "Epoch 62/100 | Train Loss: 2.5648 | Val Loss: 2.5680\n",
            "Epoch 62/100 | Train Loss: 2.5648\n",
            "Epoch 63/100 | Train Loss: 2.5751 | Val Loss: 2.5773\n",
            "Epoch 63/100 | Train Loss: 2.5751\n",
            "Epoch 64/100 | Train Loss: 2.5727 | Val Loss: 2.5623\n",
            "Epoch 64/100 | Train Loss: 2.5727\n",
            "Epoch 65/100 | Train Loss: 2.5397 | Val Loss: 2.5909\n",
            "Epoch 65/100 | Train Loss: 2.5397\n",
            "Epoch 66/100 | Train Loss: 2.5744 | Val Loss: 2.5499\n",
            "Epoch 66/100 | Train Loss: 2.5744\n",
            "Epoch 67/100 | Train Loss: 2.5575 | Val Loss: 2.5529\n",
            "Epoch 67/100 | Train Loss: 2.5575\n",
            "Epoch 68/100 | Train Loss: 2.5519 | Val Loss: 2.5630\n",
            "Epoch 68/100 | Train Loss: 2.5519\n",
            "Epoch 69/100 | Train Loss: 2.5818 | Val Loss: 2.5187\n",
            "Epoch 69/100 | Train Loss: 2.5818\n",
            "Epoch 70/100 | Train Loss: 2.5857 | Val Loss: 2.5698\n",
            "Epoch 70/100 | Train Loss: 2.5857\n",
            "Epoch 71/100 | Train Loss: 2.5531 | Val Loss: 2.5430\n",
            "Epoch 71/100 | Train Loss: 2.5531\n",
            "Epoch 72/100 | Train Loss: 2.5829 | Val Loss: 2.5353\n",
            "Epoch 72/100 | Train Loss: 2.5829\n",
            "Epoch 73/100 | Train Loss: 2.5452 | Val Loss: 2.5259\n",
            "Epoch 73/100 | Train Loss: 2.5452\n",
            "Epoch 74/100 | Train Loss: 2.5291 | Val Loss: 2.5645\n",
            "Epoch 74/100 | Train Loss: 2.5291\n",
            "Epoch 75/100 | Train Loss: 2.5261 | Val Loss: 2.5553\n",
            "Epoch 75/100 | Train Loss: 2.5261\n",
            "Epoch 76/100 | Train Loss: 2.5714 | Val Loss: 2.5267\n",
            "Epoch 76/100 | Train Loss: 2.5714\n",
            "Epoch 77/100 | Train Loss: 2.5431 | Val Loss: 2.5338\n",
            "Epoch 77/100 | Train Loss: 2.5431\n",
            "Epoch 78/100 | Train Loss: 2.5511 | Val Loss: 2.5279\n",
            "Epoch 78/100 | Train Loss: 2.5511\n",
            "Epoch 79/100 | Train Loss: 2.5692 | Val Loss: 2.5338\n",
            "Epoch 79/100 | Train Loss: 2.5692\n",
            "Epoch 80/100 | Train Loss: 2.5330 | Val Loss: 2.5333\n",
            "Epoch 80/100 | Train Loss: 2.5330\n",
            "Epoch 81/100 | Train Loss: 2.5402 | Val Loss: 2.5205\n",
            "Epoch 81/100 | Train Loss: 2.5402\n",
            "Epoch 82/100 | Train Loss: 2.5316 | Val Loss: 2.5412\n",
            "Epoch 82/100 | Train Loss: 2.5316\n",
            "Epoch 83/100 | Train Loss: 2.5217 | Val Loss: 2.5610\n",
            "Epoch 83/100 | Train Loss: 2.5217\n",
            "Epoch 84/100 | Train Loss: 2.5244 | Val Loss: 2.5422\n",
            "Epoch 84/100 | Train Loss: 2.5244\n",
            "Epoch 85/100 | Train Loss: 2.5200 | Val Loss: 2.5566\n",
            "Epoch 85/100 | Train Loss: 2.5200\n",
            "Epoch 86/100 | Train Loss: 2.5168 | Val Loss: 2.5180\n",
            "Epoch 86/100 | Train Loss: 2.5168\n",
            "Epoch 87/100 | Train Loss: 2.5420 | Val Loss: 2.5436\n",
            "Epoch 87/100 | Train Loss: 2.5420\n",
            "Epoch 88/100 | Train Loss: 2.5227 | Val Loss: 2.5382\n",
            "Epoch 88/100 | Train Loss: 2.5227\n",
            "Epoch 89/100 | Train Loss: 2.5142 | Val Loss: 2.5250\n",
            "Epoch 89/100 | Train Loss: 2.5142\n",
            "Epoch 90/100 | Train Loss: 2.5107 | Val Loss: 2.5005\n",
            "Epoch 90/100 | Train Loss: 2.5107\n",
            "Epoch 91/100 | Train Loss: 2.5147 | Val Loss: 2.5083\n",
            "Epoch 91/100 | Train Loss: 2.5147\n",
            "Epoch 92/100 | Train Loss: 2.5155 | Val Loss: 2.5341\n",
            "Epoch 92/100 | Train Loss: 2.5155\n",
            "Epoch 93/100 | Train Loss: 2.5298 | Val Loss: 2.5271\n",
            "Epoch 93/100 | Train Loss: 2.5298\n",
            "Epoch 94/100 | Train Loss: 2.5231 | Val Loss: 2.5175\n",
            "Epoch 94/100 | Train Loss: 2.5231\n",
            "Epoch 95/100 | Train Loss: 2.5278 | Val Loss: 2.5262\n",
            "Epoch 95/100 | Train Loss: 2.5278\n",
            "Epoch 96/100 | Train Loss: 2.5194 | Val Loss: 2.4863\n",
            "Epoch 96/100 | Train Loss: 2.5194\n",
            "Epoch 97/100 | Train Loss: 2.4914 | Val Loss: 2.5301\n",
            "Epoch 97/100 | Train Loss: 2.4914\n",
            "Epoch 98/100 | Train Loss: 2.4941 | Val Loss: 2.5463\n",
            "Epoch 98/100 | Train Loss: 2.4941\n",
            "Epoch 99/100 | Train Loss: 2.5057 | Val Loss: 2.5176\n",
            "Epoch 99/100 | Train Loss: 2.5057\n",
            "Epoch 100/100 | Train Loss: 2.5248 | Val Loss: 2.5084\n",
            "Epoch 100/100 | Train Loss: 2.5248\n",
            "Model saved to /content/transformer_shakespeare/output/model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/transformer_shakespeare/src && python generate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekHcuWQw7hyZ",
        "outputId": "a65b905d-eff6-4df5-c2f4-bd57fd94095b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: his Wherd, tes mlor f mf bororanorake noud hangy in he theret wl yom thakis Thee t mendore m cas.\n",
            "LI thabllontho d se onerteangounesu\n",
            "\n",
            "\n",
            "ENLowhatoul wsos thean; in st t st tce ilmea arelllliche a s fo\n"
          ]
        }
      ]
    }
  ]
}